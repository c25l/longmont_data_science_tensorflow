{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.contrib import rnn\n",
    "import tensorflow.contrib.legacy_seq2seq as seq2seq\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nodes_per_layer = 100\n",
    "layers = 2\n",
    "dropout_keep_prob = 0.99\n",
    "batch_size = 100\n",
    "num_batches =100\n",
    "num_epochs=101\n",
    "seq_length = 250\n",
    "corpus_symbols=39\n",
    "samples = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = {}\n",
    "decoder = {}\n",
    "_data = open(\"shakespeare.txt\",'r').read().lower()\n",
    "junk = {\";\":\",\",\n",
    "       \"-\":\" \",\n",
    "       \"3\":\"\",\n",
    "       \"'\":\"\",\n",
    "        \"$\":\"\",\n",
    "        \"&\":\"\",\n",
    "        \"!\":\".\",\n",
    "       \":\":\"\",\n",
    "       \"\\n\":\"\",\n",
    "       \",\":\".\"}\n",
    "\n",
    "for x in junk:\n",
    "    _data = _data.replace(x,junk[x])\n",
    "\n",
    "for xx in _data:\n",
    "    if xx not in encoder:\n",
    "        encoder[xx] = len(encoder)\n",
    "\n",
    "def data_iterator(slen, batches):\n",
    "    index=0\n",
    "    while True:\n",
    "        if (index + slen+1) > len(_data):\n",
    "            index=0\n",
    "        temp = [encoder[x] for x in _data[index:index+slen*batches+1]]\n",
    "        index += slen\n",
    "        yield (np.array(temp[:-1]).reshape(batches,slen), \n",
    "               np.array(temp[1:]).reshape(batches,slen))\n",
    "decoder = {encoder[x]:x for x in encoder}\n",
    "corpus_symbols=len(encoder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() #This resets the graph on cell run, otherwise this cell is run-once.\n",
    "sess =  tf.Session()\n",
    "#Then we'll build a deep rnn's cells, with dropout\n",
    "cells = [rnn.GRUCell(nodes_per_layer) for _ in  range(layers)]\n",
    "dropout_cells = [rnn.DropoutWrapper(cell,dropout_keep_prob) for cell in cells]\n",
    "cell = rnn.MultiRNNCell(dropout_cells, state_is_tuple=True)\n",
    "\n",
    "\n",
    "#Define our input and output data\n",
    "input_data = tf.placeholder(tf.int32, [None, None], name=\"input_data\")\n",
    "targets = tf.placeholder(tf.int32, [None, None], name = \"targets\")\n",
    "initial_state = cell.zero_state(batch_size, tf.float32)\n",
    "sampling_initial_state = cell.zero_state(1, tf.float32)\n",
    "\n",
    "#First, Let's do an embedding into the network space\n",
    "embed = tf.get_variable(\"embed\", [corpus_symbols, nodes_per_layer])\n",
    "inputs = tf.nn.embedding_lookup(embed, input_data)\n",
    "\n",
    "#dynamic rnn wrapper for cells\n",
    "gru_outputs, final_state = tf.nn.dynamic_rnn(cell, inputs, dtype=tf.float32)\n",
    "\n",
    "\n",
    "# and with this, let's deduce the outputs as well\n",
    "preds = tf.layers.dense(gru_outputs, corpus_symbols, name = \"recombine\", activation = tf.sigmoid)\n",
    "usetargets = tf.one_hot(targets, corpus_symbols)\n",
    "\n",
    "# need to define how wrong we are, and what to do about it.\n",
    "loss = tf.nn.softmax_cross_entropy_with_logits(logits=preds, labels=usetargets)\n",
    "optimizer = tf.train.AdagradOptimizer(0.02)\n",
    "train_op = optimizer.minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "data = data_iterator(seq_length, batch_size)\n",
    "for a in tqdm.tqdm_notebook(range(num_epochs)):\n",
    "    state = sess.run(initial_state)\n",
    "    for b in tqdm.tqdm_notebook(range(num_batches),leave=0):\n",
    "        x,y = next(data)\n",
    "        feed = {input_data: x, targets: y}\n",
    "        for i, _x in enumerate(initial_state):\n",
    "            feed[_x] = state[i]\n",
    "        _, state = sess.run([train_op, final_state], feed)\n",
    "    state = sess.run(sampling_initial_state)\n",
    "    nextitem = np.array([encoder[x] for x in[\"t\",\"h\",\"e\", \" \"]]).reshape(1,-1)\n",
    "    output = \"\"\n",
    "    for _ in range(samples):\n",
    "        feed = {input_data: nextitem.reshape(1,-1), targets: nextitem.reshape(1,-1)}\n",
    "        for i, _x in enumerate(sampling_initial_state):\n",
    "            feed[_x] = state[i]\n",
    "        [predicted, state] = sess.run([preds, final_state], feed)\n",
    "        proto = np.cumsum(predicted[:,-1,:])\n",
    "        nextitem = np.searchsorted(proto,np.random.rand(1)*proto[-1])\n",
    "        nextitem = np.array(nextitem)\n",
    "        output = output+ decoder[nextitem[0]]\n",
    "    print(\"sample output: \\\"{}\\\"\".format(output))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
